#!/usr/bin/env python3
"""
Schema Translator for BigQuery-Lite

Enhanced schema translator that converts BigQuery schema JSON (generated by protoc-gen-bq-schema) 
to CREATE TABLE statements for DuckDB and ClickHouse with comprehensive type support,
nested structures, and array handling.
"""

import json
import re
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ValidationError:
    """Represents a schema validation error"""
    field_path: str
    error_type: str
    message: str
    severity: str = "error"  # error, warning, info


class SchemaValidationError(Exception):
    """Exception raised when schema validation fails"""
    def __init__(self, errors: List[ValidationError]):
        self.errors = errors
        error_messages = [f"{err.field_path}: {err.message}" for err in errors]
        super().__init__(f"Schema validation failed: {'; '.join(error_messages)}")


class BaseEngineTranslator(ABC):
    """Abstract base class for engine-specific schema translators"""
    
    @abstractmethod
    def get_type_mapping(self) -> Dict[str, str]:
        """Get BigQuery to engine type mapping"""
        pass
    
    @abstractmethod
    def format_nullable_type(self, base_type: str, is_nullable: bool) -> str:
        """Format nullable type for the engine"""
        pass
    
    @abstractmethod
    def format_array_type(self, element_type: str) -> str:
        """Format array type for the engine"""
        pass
    
    @abstractmethod
    def format_record_type(self, field_definitions: List[str]) -> str:
        """Format nested record/struct type for the engine"""
        pass
    
    @abstractmethod
    def format_create_table_clause(self, table_name: str, column_definitions: List[str]) -> str:
        """Format the complete CREATE TABLE statement"""
        pass


class DuckDBTranslator(BaseEngineTranslator):
    """DuckDB-specific schema translator"""
    
    def get_type_mapping(self) -> Dict[str, str]:
        return {
            'STRING': 'VARCHAR',
            'INTEGER': 'BIGINT',
            'INT64': 'BIGINT',
            'FLOAT': 'DOUBLE',
            'FLOAT64': 'DOUBLE',
            'BOOLEAN': 'BOOLEAN',
            'TIMESTAMP': 'TIMESTAMP',
            'DATE': 'DATE',
            'TIME': 'TIME',
            'DATETIME': 'TIMESTAMP',
            'BYTES': 'BLOB',
            'NUMERIC': 'DECIMAL(38,9)',
            'BIGNUMERIC': 'DECIMAL(76,38)',
            'JSON': 'JSON',
            'GEOGRAPHY': 'VARCHAR',  # Store as WKT string
        }
    
    def format_nullable_type(self, base_type: str, is_nullable: bool) -> str:
        """DuckDB handles NULLs naturally, no special nullable wrapper needed"""
        return base_type
    
    def format_array_type(self, element_type: str) -> str:
        """DuckDB array syntax: TYPE[]"""
        return f"{element_type}[]"
    
    def format_record_type(self, field_definitions: List[str]) -> str:
        """DuckDB STRUCT syntax: STRUCT(field1 TYPE1, field2 TYPE2, ...)"""
        return f"STRUCT({', '.join(field_definitions)})"
    
    def format_create_table_clause(self, table_name: str, column_definitions: List[str]) -> str:
        """Generate DuckDB CREATE TABLE statement"""
        columns_sql = ',\n    '.join(column_definitions)
        return f"""CREATE TABLE IF NOT EXISTS {table_name} (
    {columns_sql}
);"""


class ClickHouseTranslator(BaseEngineTranslator):
    """ClickHouse-specific schema translator"""
    
    def get_type_mapping(self) -> Dict[str, str]:
        return {
            'STRING': 'String',
            'INTEGER': 'Int64',
            'INT64': 'Int64',
            'FLOAT': 'Float64',
            'FLOAT64': 'Float64',
            'BOOLEAN': 'Bool',
            'TIMESTAMP': 'DateTime64(3)',  # Millisecond precision
            'DATE': 'Date',
            'TIME': 'String',  # ClickHouse doesn't have native TIME type
            'DATETIME': 'DateTime64(3)',
            'BYTES': 'String',  # Base64 encoded string
            'NUMERIC': 'Decimal(38,9)',
            'BIGNUMERIC': 'Decimal(76,38)',
            'JSON': 'String',  # JSON as string in ClickHouse
            'GEOGRAPHY': 'String',  # Store as WKT string
        }
    
    def format_nullable_type(self, base_type: str, is_nullable: bool) -> str:
        """ClickHouse nullable wrapper: Nullable(TYPE)"""
        if is_nullable:
            return f"Nullable({base_type})"
        return base_type
    
    def format_array_type(self, element_type: str) -> str:
        """ClickHouse array syntax: Array(TYPE)"""
        return f"Array({element_type})"
    
    def format_record_type(self, field_definitions: List[str]) -> str:
        """ClickHouse Tuple syntax for nested records"""
        # For named tuples, we could use: Tuple(field1 TYPE1, field2 TYPE2, ...)
        # But standard tuples are more compatible: Tuple(TYPE1, TYPE2, ...)
        field_types = []
        for field_def in field_definitions:
            # Extract just the type part from "field_name TYPE"
            type_part = field_def.split(' ', 1)[-1]
            field_types.append(type_part)
        return f"Tuple({', '.join(field_types)})"
    
    def format_create_table_clause(self, table_name: str, column_definitions: List[str]) -> str:
        """Generate ClickHouse CREATE TABLE statement with MergeTree engine"""
        columns_sql = ',\n    '.join(column_definitions)
        return f"""CREATE TABLE IF NOT EXISTS {table_name} (
    {columns_sql}
) ENGINE = MergeTree()
ORDER BY tuple();"""


class SchemaTranslator:
    """
    Enhanced schema translator for BigQuery-Lite
    
    Converts BigQuery schema JSON to CREATE TABLE statements for multiple engines
    with support for nested records, arrays, and comprehensive type mapping.
    """
    
    def __init__(self):
        self.engines = {
            'duckdb': DuckDBTranslator(),
            'clickhouse': ClickHouseTranslator()
        }
    
    def validate_schema_json(self, schema_json: List[Dict[str, Any]]) -> List[ValidationError]:
        """
        Validate BigQuery schema JSON structure
        
        Args:
            schema_json: List of field definitions
            
        Returns:
            List of validation errors (empty if valid)
        """
        errors = []
        
        if not isinstance(schema_json, list):
            errors.append(ValidationError(
                field_path="root",
                error_type="type_error",
                message="Schema must be a list of field definitions"
            ))
            return errors
        
        if not schema_json:
            errors.append(ValidationError(
                field_path="root",
                error_type="empty_schema",
                message="Schema cannot be empty"
            ))
            return errors
        
        # Validate each field recursively
        seen_names = set()
        for i, field in enumerate(schema_json):
            field_path = f"field[{i}]"
            field_errors = self._validate_field(field, field_path, seen_names)
            errors.extend(field_errors)
        
        return errors
    
    def _validate_field(self, field: Dict[str, Any], field_path: str, seen_names: set) -> List[ValidationError]:
        """Validate a single field definition"""
        errors = []
        
        # Check required attributes
        if not isinstance(field, dict):
            errors.append(ValidationError(
                field_path=field_path,
                error_type="type_error", 
                message="Field must be a dictionary"
            ))
            return errors
        
        # Required: name
        if 'name' not in field:
            errors.append(ValidationError(
                field_path=field_path,
                error_type="missing_attribute",
                message="Field missing required 'name' attribute"
            ))
        else:
            field_name = field['name']
            if not isinstance(field_name, str) or not field_name.strip():
                errors.append(ValidationError(
                    field_path=f"{field_path}.name",
                    error_type="invalid_value",
                    message="Field name must be a non-empty string"
                ))
            else:
                # Check for duplicate names
                sanitized_name = self._sanitize_field_name(field_name)
                if sanitized_name in seen_names:
                    errors.append(ValidationError(
                        field_path=f"{field_path}.name",
                        error_type="duplicate_name",
                        message=f"Duplicate field name: {sanitized_name}"
                    ))
                seen_names.add(sanitized_name)
        
        # Required: type
        if 'type' not in field:
            errors.append(ValidationError(
                field_path=field_path,
                error_type="missing_attribute",
                message="Field missing required 'type' attribute"
            ))
        else:
            field_type = field['type']
            if not isinstance(field_type, str):
                errors.append(ValidationError(
                    field_path=f"{field_path}.type",
                    error_type="invalid_value",
                    message="Field type must be a string"
                ))
            else:
                # Validate type against known BigQuery types
                valid_types = {
                    'STRING', 'INTEGER', 'INT64', 'FLOAT', 'FLOAT64', 'BOOLEAN',
                    'TIMESTAMP', 'DATE', 'TIME', 'DATETIME', 'BYTES',
                    'NUMERIC', 'BIGNUMERIC', 'JSON', 'GEOGRAPHY', 'RECORD'
                }
                if field_type not in valid_types:
                    errors.append(ValidationError(
                        field_path=f"{field_path}.type",
                        error_type="invalid_type",
                        message=f"Unknown BigQuery type: {field_type}",
                        severity="warning"
                    ))
        
        # Optional: mode
        if 'mode' in field:
            mode = field['mode']
            valid_modes = {'REQUIRED', 'NULLABLE', 'REPEATED'}
            if mode not in valid_modes:
                errors.append(ValidationError(
                    field_path=f"{field_path}.mode",
                    error_type="invalid_value",
                    message=f"Invalid mode: {mode}. Must be one of {valid_modes}"
                ))
        
        # Validate nested fields for RECORD type
        if field.get('type') == 'RECORD':
            if 'fields' not in field:
                errors.append(ValidationError(
                    field_path=f"{field_path}.fields",
                    error_type="missing_attribute",
                    message="RECORD type must have 'fields' attribute"
                ))
            else:
                nested_fields = field['fields']
                if not isinstance(nested_fields, list) or not nested_fields:
                    errors.append(ValidationError(
                        field_path=f"{field_path}.fields",
                        error_type="invalid_value",
                        message="RECORD fields must be a non-empty list"
                    ))
                else:
                    # Recursively validate nested fields
                    nested_seen_names = set()
                    for j, nested_field in enumerate(nested_fields):
                        nested_path = f"{field_path}.fields[{j}]"
                        nested_errors = self._validate_field(nested_field, nested_path, nested_seen_names)
                        errors.extend(nested_errors)
        
        return errors
    
    def _sanitize_field_name(self, name: str) -> str:
        """Sanitize field name for SQL compatibility"""
        # Replace invalid characters with underscores
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        
        # Ensure it starts with a letter or underscore
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"_{sanitized}"
        
        # Handle reserved keywords by adding suffix
        reserved_keywords = {
            'select', 'from', 'where', 'group', 'order', 'having', 
            'insert', 'update', 'delete', 'create', 'drop', 'alter',
            'table', 'column', 'index', 'view', 'database', 'schema',
            'and', 'or', 'not', 'null', 'true', 'false', 'case', 'when',
            'then', 'else', 'end', 'if', 'exists', 'between', 'like',
            'in', 'is', 'as', 'on', 'join', 'left', 'right', 'inner',
            'outer', 'union', 'all', 'distinct', 'limit', 'offset'
        }
        
        if sanitized.lower() in reserved_keywords:
            sanitized += "_field"
        
        return sanitized
    
    def _convert_field_type(self, 
                          field: Dict[str, Any], 
                          translator: BaseEngineTranslator) -> str:
        """
        Convert a BigQuery field to engine-specific type
        
        Args:
            field: BigQuery field definition
            translator: Engine-specific translator
            
        Returns:
            Engine-specific type string
        """
        field_type = field['type']
        mode = field.get('mode', 'NULLABLE')
        
        # Handle RECORD type (nested structures)
        if field_type == 'RECORD' and 'fields' in field:
            nested_field_defs = []
            for nested_field in field['fields']:
                nested_name = self._sanitize_field_name(nested_field['name'])
                nested_type = self._convert_field_type(nested_field, translator)
                nested_field_defs.append(f"{nested_name} {nested_type}")
            
            base_type = translator.format_record_type(nested_field_defs)
        else:
            # Handle primitive types
            type_mapping = translator.get_type_mapping()
            base_type = type_mapping.get(field_type, 'String')  # Default fallback
        
        # Handle REPEATED mode (arrays)
        if mode == 'REPEATED':
            base_type = translator.format_array_type(base_type)
            # Arrays are inherently nullable in most systems
            return base_type
        
        # Handle nullable vs required
        is_nullable = (mode == 'NULLABLE')
        return translator.format_nullable_type(base_type, is_nullable)
    
    def generate_create_table_sql(self, 
                                schema_json: List[Dict[str, Any]], 
                                table_name: str,
                                engine: str,
                                database_name: Optional[str] = None) -> str:
        """
        Generate CREATE TABLE SQL statement from BigQuery schema
        
        Args:
            schema_json: BigQuery schema as list of field definitions
            table_name: Target table name
            engine: Target engine ("duckdb" or "clickhouse")
            database_name: Optional database name
            
        Returns:
            SQL CREATE TABLE statement
            
        Raises:
            SchemaValidationError: If schema validation fails
            ValueError: If engine is not supported
        """
        # Validate schema
        validation_errors = self.validate_schema_json(schema_json)
        error_level_errors = [err for err in validation_errors if err.severity == "error"]
        if error_level_errors:
            raise SchemaValidationError(error_level_errors)
        
        # Log warnings
        warnings = [err for err in validation_errors if err.severity == "warning"]
        for warning in warnings:
            logger.warning(f"Schema warning: {warning.field_path}: {warning.message}")
        
        # Get engine translator
        if engine not in self.engines:
            raise ValueError(f"Unsupported engine: {engine}. Supported engines: {list(self.engines.keys())}")
        
        translator = self.engines[engine]
        
        # Convert all fields to column definitions
        column_definitions = []
        for field in schema_json:
            field_name = self._sanitize_field_name(field['name'])
            field_type = self._convert_field_type(field, translator)
            mode = field.get('mode', 'NULLABLE')
            
            # Build column definition
            col_def = f"{field_name} {field_type}"
            
            # Add NOT NULL constraint for required fields (except arrays which handle nullability differently)
            if mode == 'REQUIRED' and mode != 'REPEATED':
                col_def += " NOT NULL"
            
            column_definitions.append(col_def)
        
        # Build full table name
        full_table_name = table_name
        if database_name:
            full_table_name = f"{database_name}.{table_name}"
        
        # Generate CREATE TABLE statement
        return translator.format_create_table_clause(full_table_name, column_definitions)
    
    def generate_flattened_view_sql(self, 
                                  schema_json: List[Dict[str, Any]], 
                                  table_name: str,
                                  engine: str,
                                  database_name: Optional[str] = None) -> Optional[str]:
        """
        Generate a flattened SQL view for schemas with nested RECORD fields
        
        Args:
            schema_json: BigQuery schema as list of field definitions
            table_name: Source table name
            engine: Target engine ("duckdb" or "clickhouse")
            database_name: Optional database name
            
        Returns:
            SQL CREATE VIEW statement or None if no nested fields
        """
        # Check if schema has nested fields
        has_nested = any(field.get('type') == 'RECORD' for field in schema_json)
        if not has_nested:
            return None
        
        # Build full table name
        full_table_name = table_name
        if database_name:
            full_table_name = f"{database_name}.{table_name}"
        
        view_name = f"{full_table_name}_flattened"
        
        # Generate flattened SELECT clause
        select_fields = []
        for field in schema_json:
            field_name = self._sanitize_field_name(field['name'])
            
            if field.get('type') == 'RECORD' and 'fields' in field:
                # Flatten nested record fields
                nested_fields = self._generate_flattened_fields(field, field_name, engine)
                select_fields.extend(nested_fields)
            else:
                # Include non-nested fields as-is
                select_fields.append(field_name)
        
        select_clause = ',\n    '.join(select_fields)
        
        return f"""CREATE VIEW IF NOT EXISTS {view_name} AS
SELECT 
    {select_clause}
FROM {full_table_name};"""
    
    def _generate_flattened_fields(self, 
                                 record_field: Dict[str, Any], 
                                 field_prefix: str, 
                                 engine: str) -> List[str]:
        """Generate flattened field access for nested records"""
        flattened_fields = []
        
        for nested_field in record_field.get('fields', []):
            nested_name = self._sanitize_field_name(nested_field['name'])
            
            if engine == 'duckdb':
                # DuckDB struct access: field.nested_field
                field_access = f"{field_prefix}.{nested_name}"
            else:  # clickhouse
                # ClickHouse tuple access: field.1, field.2, etc.
                # Note: This is simplified - real implementation would need field position mapping
                field_access = f"{field_prefix}.{nested_name}"
            
            # Create alias for flattened field
            alias = f"{field_prefix}_{nested_name}"
            flattened_fields.append(f"{field_access} AS {alias}")
            
            # Recursively handle nested records
            if nested_field.get('type') == 'RECORD':
                recursive_fields = self._generate_flattened_fields(
                    nested_field, f"{field_prefix}_{nested_name}", engine
                )
                flattened_fields.extend(recursive_fields)
        
        return flattened_fields
    
    def generate_insert_template(self, 
                               schema_json: List[Dict[str, Any]], 
                               table_name: str,
                               database_name: Optional[str] = None) -> str:
        """
        Generate INSERT template for data ingestion
        
        Args:
            schema_json: BigQuery schema as list of field definitions
            table_name: Target table name
            database_name: Optional database name
            
        Returns:
            SQL INSERT template with placeholders
        """
        # Validate schema
        validation_errors = self.validate_schema_json(schema_json)
        error_level_errors = [err for err in validation_errors if err.severity == "error"]
        if error_level_errors:
            raise SchemaValidationError(error_level_errors)
        
        field_names = [self._sanitize_field_name(field['name']) for field in schema_json]
        
        # Build full table name
        full_table_name = table_name
        if database_name:
            full_table_name = f"{database_name}.{table_name}"
        
        placeholders = ', '.join(['?' for _ in field_names])
        columns = ', '.join(field_names)
        
        return f"INSERT INTO {full_table_name} ({columns}) VALUES ({placeholders})"


def main():
    """Example usage of enhanced SchemaTranslator"""
    
    # Complex example schema with nested records and arrays
    example_schema = [
        {
            "name": "id",
            "type": "INTEGER",
            "mode": "REQUIRED"
        },
        {
            "name": "user_profile",
            "type": "RECORD",
            "mode": "NULLABLE",
            "fields": [
                {
                    "name": "name",
                    "type": "STRING",
                    "mode": "REQUIRED"
                },
                {
                    "name": "email",
                    "type": "STRING", 
                    "mode": "NULLABLE"
                },
                {
                    "name": "preferences",
                    "type": "RECORD",
                    "mode": "NULLABLE",
                    "fields": [
                        {
                            "name": "theme",
                            "type": "STRING",
                            "mode": "NULLABLE"
                        },
                        {
                            "name": "notifications_enabled",
                            "type": "BOOLEAN",
                            "mode": "NULLABLE"
                        }
                    ]
                }
            ]
        },
        {
            "name": "tags",
            "type": "STRING",
            "mode": "REPEATED"
        },
        {
            "name": "created_at",
            "type": "TIMESTAMP",
            "mode": "REQUIRED"
        },
        {
            "name": "is_active",
            "type": "BOOLEAN",
            "mode": "NULLABLE"
        }
    ]
    
    print("=== Enhanced BigQuery Schema Translation ===\n")
    
    translator = SchemaTranslator()
    
    # Validate schema
    print("Schema Validation:")
    print("-" * 50)
    validation_errors = translator.validate_schema_json(example_schema)
    if validation_errors:
        for error in validation_errors:
            severity_icon = "❌" if error.severity == "error" else "⚠️"
            print(f"{severity_icon} {error.field_path}: {error.message}")
    else:
        print("✅ Schema validation passed")
    print()
    
    # Test DuckDB translation
    print("DuckDB CREATE TABLE:")
    print("-" * 50)
    try:
        duckdb_sql = translator.generate_create_table_sql(
            example_schema, "users", "duckdb", "bigquery_lite"
        )
        print(duckdb_sql)
    except Exception as e:
        print(f"❌ Error: {e}")
    print()
    
    # Test ClickHouse translation
    print("ClickHouse CREATE TABLE:")
    print("-" * 50)
    try:
        clickhouse_sql = translator.generate_create_table_sql(
            example_schema, "users", "clickhouse", "bigquery_lite"
        )
        print(clickhouse_sql)
    except Exception as e:
        print(f"❌ Error: {e}")
    print()
    
    # Test flattened view generation
    print("DuckDB Flattened View:")
    print("-" * 50)
    try:
        flattened_sql = translator.generate_flattened_view_sql(
            example_schema, "users", "duckdb", "bigquery_lite"
        )
        if flattened_sql:
            print(flattened_sql)
        else:
            print("No nested fields found - flattened view not needed")
    except Exception as e:
        print(f"❌ Error: {e}")
    print()
    
    # Test INSERT template
    print("INSERT Template:")
    print("-" * 50)
    try:
        insert_template = translator.generate_insert_template(
            example_schema, "users", "bigquery_lite"
        )
        print(insert_template)
    except Exception as e:
        print(f"❌ Error: {e}")


if __name__ == "__main__":
    main()